---
title: "Class8"
author: "renyan"
date: "2019年12月6日"
output: 
  prettydoc::html_pretty:
    theme: leonids
    highlight: github
---

```{r, include = F}
library(prettydoc)
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r global_options, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
```

今儿讲文本分析！！！


# 字句统计

```{r}
library(jiebaRD)
library(jiebaR)
library(ggplot2)

setwd("C:/Users/lenovo/Desktop/RYaan/R/class_software/Notes")
com_data <- read.csv("../Data/Apple7_comments.csv", header = T, stringsAsFactors = F, encoding = "GB2312")
dim(com_data)
ratings <- factor(com_data$rating)

```

```{r}
# 将评论数据单独提取出
comments <- com_data[,6]
head(comments)
```


## 进行简单统计

- 注意：```nchar()``` 将汉字和中文标点都当做一个字符处理

```{r}
# 统计每条评论字数
words <- nchar(comments)
# 查看前两条评论数据
comments[1:2]
# 查看前两条评论的字数
words[1:2]
sentences <- strsplit(comments, split = "，|。|！|？")
# 计算以句子为单位，每条评论的长度，即每个评论有几句话
Lsen <- lapply(sentences, length)
# 去列表化
Lsen <- unlist(Lsen)
# 绘图
boxplot(words~ratings, data = com_data, ylab = '评分', xlab = '字数')
boxplot(Lsen~ratings, data = com_data, ylab = "评分", xlab = "句子数")
```

评分低的评论词数偏多，句子也越多。



# 中文分词

## 初始化分词器

初始化分词器后既能进行分词又可以进行词性标注，去除停用词等。  
- ```worker()``` 中的参数解释：  
  - ```bylines```: 逐行进行分词操作等
  - ``````: 
- ```cutter``` 后面是hi方括号不是圆括号

```{r}
# 逐行分词，初始化分词器
library(jiebaR)
cutter <- worker(bylines = TRUE)
res <- cutter[comments]
head(res)
```


## 优化词库

### 去掉停用词

```{r}
# 里面存的是一些没有实际意义的词语
stoppath <- "../Data/stopwords.dat"
cutter <- worker(bylines = TRUE, stop_word = stoppath)
res_stop <- cutter[comments]
head(res_stop)
```

### 自定义词典

```{r}
library(devtools)
library(cidian)
```

显示默认的词典路径

```{r}
# 显示词典位置
syspath = show_dictpath()
```

jieba.dict.utf8, 默认的系统字典，第一列为词项，第二列为词频，第三列为词性标记

```{r}
sys_user_path = file.path(syspath, "user.dict.utf8")
sys_user_path
```

- 方法一：复制粘贴（可忽略）

把新词直接复制粘贴到默认的词典中

- 方法二：R 语句添加特定的词（可忽略）  

```{r}
# Sys.setlocale('LC_ALL','C')
sys_user_dict <- load_user_dict(filePath = sys_user_path)
add_user_words(dict = sys_user_dict, words = c("老人机", "双十一"), tags = c("n", "n"))
```


- 方法三

直接建立自己的自定义字典，可以直接用搜狗细胞词库进行转化。

```{r}
decode_scel(scel = "../Data/phone_words.scel", output = "../Data/phone_words_sogou.txt")
dictpath = "../Data/phone_words_sogou.txt"
cutter <- worker(bylines = TRUE, user = dictpath)
res_sogou <- cutter[comments]
head(res_sogou)
```




# 词性标注

## 提取每个词的词性

- 每条评论里的每个词都被标记了一个词性

```{r}
cutter <- worker(bylines = TRUE, "tag")
res_tag <- cutter[comments]
res_tag[1:2]
# 提取每个词的词性
tag <- lapply(res_tag, names)
tag[1:2]
```

## 按照词性进行筛选

- 注意要先将 list 类转化为 matrix 类才能继续进行选择操作

```{r}
# 把list里的每个元素都转化为一个矩阵，即每条评论对应一个矩阵
text <- lapply(res_tag, as.matrix)
tag <- lapply(tag, as.matrix)
tag[[1]]
text[[1]]
```

```{r}
# 将每行文本的分词结果逐一排列起来
text_all <- as.data.frame(do.call(rbind, text), stringsAsFactors = F)
# 将标注的词性结果逐一排列起来
tag_all <- as.data.frame(do.call(rbind, tag), stringsAsFactors = F)
head(text_all)
head(tag_all)
```

## 对分词结果按照词性进行筛选`


```{r}
# 读入选取的词性
choose_tags <- read.csv("../Data/tags.csv", header = F, stringsAsFactors = F)
#找词性所在列
choose_tags <- as.matrix(choose_tags[, 1])
# 找出tag中属于choose_tags中任意一个词性的下标
tnum <- apply(choose_tags, 1, function(x){return(which(tag_all[,1]==x))})
# 将tnum从list转为vector
tnum <- unlist(tnum)
# 得到筛选后的tag和text
tag_all2 = tag_all[tnum, 1]
text_all2 <- text_all[tnum, 1]
```

# 提取关键词

## 对选出的次进行二次筛选

```{r}
# 找出每个词的长度
Len <- nchar(text_all2)
# 选出长度大于一的词（先找出其下标）
num <- which(Len>1)
# 得到筛选后的tag和text
tag_all3 <- tag_all2[num]
text_all3 <- text_all2[num]
# 观察选出来的词一共有多少个
unique_phrases <- unique(text_all3)
length(unique_phrases)
```



## 选取高频词作为关键词

```{r}
# 进行词频统计
freq <- as.data.frame(table(text_all3), stringsAsFactors = F)
# 将词频个数进行降序排列
freq <- freq[order(-freq[,2]),]
# 挑选前20个高频词
top20 <- freq[1:20,]
top20
```

## 用TF-IDF提取关键词

- 使用jieba要注意要将评论单独存为一个文件

```{r}
cutter <- worker("keywords", bylines = TRUE, topn = 20)
# 评论必须单独保存为一个文件
write.table(comments, file = "../Data/comments.txt", quote = F, row.names = F, col.names = F)
key = keywords(code = "../Data/comments.txt", cutter)
key
```

和使用高频词的结论相似


# 制作词云


```{r}
library(RColorBrewer)
library(wordcloud)
```
```{r}
# 挑选前100个次制作图云
top100 <- freq[1:100,]
colnames(top100) <- c("phrases", "freq")
# 制作图云
# 设置颜色系（调用调色板 Dark2 中的8个颜色
mycolors <- brewer.pal(8, "Dark2")
windowsFonts(myFont = windowsFont("华文彩云"))
wordcloud(top100$phrases, top100$freq, random.order = F, random.color = F, colors = mycolors, family = "myFont")
```


## 利用 tagxedo 做词云

```{r}
write.table(top100, file = "../Data/Top100.txt", quote = F, row.names = F, col.names = F)
```




































