# 随时总结

---

# 实用小工具

- `id()` 用于查看两个变量是否使用的同一块内存地址

- 追踪 size 的变化

```{python}
from torchsummary import summary
summary(net, (1, 32, 32))
```

- 格式化输出

```{python}
true_name = 'renyan'
nick_name = 'fenchangyin'
print("I'm {}, but you can also call me {}".format(true_name, nick_name))
print("I'm {0}, but you can also call me {1}. Remember me: {0}".format(true_name, nick_name))
```

- 设备

```{python}
device = torch.device("cuda" if torch.cuda.is_available() else 'cpu')
```

- `tqdm` 进度条使用

```{python}
from tqdm import tqdm
# 正确
for i, (img, lbl) in enumerate(tqdm(iter(self.test_loader))):
    pass
# 错误显示
for i, (img, lbl) in tqdm(enumerate(iter(self.test_loader))):
    pass
```

- `ndim`直查看维数，不要再使用 `len(tensor.shape)`



# 工具使用

## 数据集准备

- 准备操作流程的列表，列表中的元素为 `torchvision.transform.`
- `transform = torchvision.transforms.Compose(list)`
- `data_iter = torch.utils.data.DataLoader(data, batch_size, shuffle)`

```{python}
def load_data_fashion_mnist_resize(batch_size, resize = None, root = './data/'):
    trans = []
    if resize:
        trans.append(torchvision.transforms.Resize(size = resize))
    trans.append(torchvision.transforms.ToTensor())
    
    transform = torchvision.transforms.Compose(trans)
    mnist_train = torchvision.datasets.FashionMNIST(root = root, train = True, download = False, transform = transform)
    mnist_test = torchvision.datasets.FashionMNIST(root = root, train = False, download = False, transform = transform)
    train_iter = torch.utils.data.DataLoader(mnist_train, batch_size = batch_size, shuffle = True)
    test_iter = torch.utils.data.DataLoader(mnist_test, batch_size = batch_size, shuffle = False)
    
    return train_iter, test_iter
```



## 优化器使用

三步骤

```{python}
optimizer.zero_grad()
loss.backward()
optimizer.step()
```

- `optimizer.step()` 必不可少，否则就算计算得到了梯度也无法更新网络中的参数

## trochvision

计算机视觉常用

- torchvision.datasets： 常用数据集的接口
- torchvision.models：常用的模型结构
- torchvision.transforms：常用的图像变换
- torchvision.utils：其他常用方法

## 常用功能所在模块

- DataLoader：torch.utils.data.DataLoader



## 学习率调整

- 相关函数所在模块：`torch.optim.lr_scheduler`

- 相关详细解释：https://blog.csdn.net/qyhaill/article/details/103043637

  ```{python}
  # 每进行 step_size 轮训练，就把学习率调整为之前的 gamma 倍
  scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 5, gamma = 0.5)
  ```

  



---

# 思路整理

## 手动搭建

- Step1：确定输入输出神经元的个数
- Step2：初始化模型参数
  - 根据计算公式给出参数的维度，并据此初始化
  - 设置 `requires_grad = True`





---

# 深入理解

## dim 参数

- 很多函数中都有 `dim` 参数可以调节，`dim = 0` 表示所有操作都把 Tensor 最终向第 0 维的方向进行压缩
  - `dim = 0`，纵向计算，最后得到行向量
  - `dim = 1`， 横向计算，最后得到列向量
    - `keepdim = True`，才可以看到真正的计算维度，否则 Pytorch 会默认对计算结果进行降维，最后看到的总是行向量



# 常见问题

## 过拟合

> 过拟合是模型相对于样本比较复杂，是一个相对概念，需要参考样本量的大小确定模型的参数量是否过于复杂

- 方法一：进行权重衰减
  - 可以设置 `optimizer` 中的 `weight_decay` 超参数
- 方法二：使用丢弃法
  - 训练时的正则化方法，但是在测试时不应该使用丢弃法

## 梯度问题

> 主要指：梯度衰减，梯度爆炸（网络层数过多时容易出现这样的问题）

- 初始化：
  - 不要自己手动初始化，比如若全部初始化为相同的值，神经网络失效，实际上相当于只有一个隐藏单元
  - pytorch 不同 Layer 给出了不同的初始化策略，直接使用即可
  - Xavier 随机初始化
    - 每层输出的方差不该受该层输入个数影响
    - 每层梯度的方差不该受该层输出个数的影响