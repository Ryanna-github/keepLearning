# Modul 类

> 所在位置：torch.nn

## 主要函数用法

```{python}
import torch
from torch import nn

class MLP(nn.Module):
    def __init__(self):
        # 对父类进行初始化方法
        super(MLP, self).__init__()
        self.hidden = nn.Linear(input_num, hidden_num)
        # 隐藏层的激活函数
        self.act = nn.ReLU()
        self.output = nn.Linear(hidden_num, output_num)
        
    # 向前传播算法
    def forward(self, x):
        a = self.act(self.hidden(x))
        return self.output(a)
```

## 子类 `Sequential`

```{python}
class MySequential(nn.Module):
    from collections import OrderedDict
    def __init__(self, *args):
        super(MySequential, self).__init__()
        if(len(args) == 1 and isinstance(args[0], OrderedDict)):
            for key, module in args[0].items():
                self.add_module(key, module)
        else:
            for idx, module in enumerate(args):
                self.add_module(str(idx), module)
    
    def forward(self, input):
        for module in self._modules.values():
            input = module(input)
        return input
```

- `add_module` 可以依次添加模块
- 任何 `Module` 子类都可以使用 `_modules` 查看其中的模块细节信息

```{python}
# 传入 OrderedDict 情况
from collections import OrderedDict
od = OrderedDict([
    ("hidden", nn.Linear(input_num, hidden_num)),
    ("act", nn.ReLU()),
    ("output", nn.Linear(hidden_num, output_num))
])
net = MySequential(od)
# 传入多个 Module 情况
# 没有命名则自动从数字0开始命名
net = MySequential(nn.Linear(input_num, hidden_num),
                  nn.ReLU(),
                  nn.Linear(hidden_num, output_num))
```



## 子类 `ModuleList`

```{python}
# 初始化
net = nn.ModuleList([nn.Linear(input_num, hidden_num), nn.ReLU()])
# 添加元素
net.append(nn.Linear(hidden_num, output_num))
```



## 子类 `ModuleDict`

```{python}
# 初始化
net = nn.ModuleDict({
    'linear': nn.Linear(input_num, hidden_num),
    'act': nn.ReLU()
})
# 添加元素
net['output'] = nn.Linear(hidden_num, output_num)
```

---

# Tensor 类

> 基本用法见“Pytorch 学习笔记.md”

## 子类 `Parameter`

- `Tensor` 有的属性其均有
  - `.data`
  - `.grad`
- `nn.Parameter` 定义的张量会被自动添加到参数列表，使用 `nn.paramteres()` 或者 `nn.named_paramters()` 可以进行访问
- 初始化方法：调用 `torch.nn.init` 模块（不记录梯度地更新张量的值）
  - `init.normal_()`
  - `init.constant_()`
  - 自定义函数，注意张量的改变不可以更新其梯度值
    - 方法一：`with torch.no_grad()`
    - 方法二：直接修改 `param.data`

- 共享模型参数

## 子类 `ParameterList`

- 是一个包含多个 `nn.Parameter` 类对象的列表，可以使用 `.append()`，`.extend()` 在列表后增加参数

## 子类 `ParameterDict`

- 新增元素方式 `.update()`
- 用法示例

```{python}
class MyDictDense(nn.Module):
    def __init__(self):
        super(MyDictDense, self).__init__()
        self.params = nn.ParameterDict({
            'linear1': nn.Parameter(torch.randn(4, 4)),
            'linear2': nn.Parameter(torch.randn(4, 1))
        })
        self.params.update({
            'linear3': nn.Parameter(torch.randn(4, 2))
        })
    
    def forward(self, x, choice = "linear1"):
        return torch.mm(x, self.params[choice])
```



# 学习率调整

使用函数所在包：`torch.optim.lr_scheduler`

- `torch.optim.lr_scheduler.StepLR(optimizer, step_size, gamma=0.1, last_epoch=-1)`
  - 每过 step size 个epoch 就进行一次更新：这就是踩坑的地方！实际上是每调用 step_size 次 `scheduler.step()`，才会更新一次学习率
  - 更新方法为学习率变为以前的 gamma 倍



# 读写

## Tensor 读写

```{python}
# 存储
torch.save(x, 'x.pt')
torch.save([x, y], 'xy.pt')
torch.save({'x': x, 'y': y})
# 读取
x2 = torch.load('x.pt')
xy_list = torch.load('xy.pt')
xy = torch.load('xy_dict.pt')
```

## 模型读写

```{python}
# 保存模型参数
torch.save(model.state_dict(), PATH)
# 加载模型参数(model 先初始化)
model.load_state_dict(torch.load(PATH))
# 保存真个模型
torch.save(model, PATH)
# 加载整个模型(model 不需要先初始化)
model = torch.load(PATH)
```

---

# 卷积

## 卷积操作

设原始张量宽 $n_w$，高 $n_h$，核宽 $k_w$，高 $k_h$，padding 在两个方向上填充的个数分别为 $p_w, p_h$

- 输出张量的大小为：$(n_h - k_h + p_h + 1)\times(n_w - k_w + p_w + 1)$
- $k$ 为奇数时两侧均填充 $\frac{k-1}{2}$，即可使得输出向量与输入向量同形

再增加步幅 $s_w, s_h$

- 输出张量的大小为：$(n_h - k_h + p_h + s_h)/s_h\times(n_w - k_w + p_w + s_w)/s_w$

## 多通道情况

- 多通道输入：将各个通道的卷积计算结果相加即可，卷积核的大小为 $c_i\times k_h\times k_w$
- 多通道输出：多个卷积核分别运算，卷积核的大小为 $c_o\times c_i\times k_h\times k_w$

## 1x1 卷积核

- 与全连接层等价，相当于对各个颜色通道做不同的加权和，每一种加权方式都会得到一个输出张量

## 池化

- 为了缓解卷积对位置的过度敏感性

---

# 批量归一化

## 对全连接层的批量归一化

- 归一是不同样本点之间的概念，对群体进行归一化操作
- 批量归一化不改变各个向量的维数
- 如果批量归一化无益，理论上学出的模型可以做到仿佛没有批量归一化
- 具体步骤
1. $x = Wu + b$
2. $y^{(i)} = BN(x^{(i)})$
   1. $\mu_{\mathcal{B}} \leftarrow \frac{1}{m} \sum_{i=1}^{m} \boldsymbol{x}^{(i)}, \sigma_{\mathcal{B}}^{2} \leftarrow \frac{1}{m} \sum_{i=1}^{m}\left(\boldsymbol{x}^{(i)}-\boldsymbol{\mu}_{\mathcal{B}}\right)^{2}$
   2. $\hat{x}^{(i)} \leftarrow \frac{x^{(i)}-\mu_{\mathcal{B}}}{\sqrt{\sigma_{\mathcal{B}}^{2}+\epsilon}}$
   3. $y^{(i)} \leftarrow \gamma \odot \hat{x}^{(i)}+\beta$

## 对卷积层做批量归一化

- 发生在卷积计算后，激活函数**前**，对每个通道分别做批量归一化，每个通道都有独立的标量拉伸和偏移参数

- 训练模式和预测模式下的计算结果不相同（原理同丢弃法）

